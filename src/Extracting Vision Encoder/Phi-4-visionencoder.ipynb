{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ninja in /home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages (1.11.1.4)\n",
      "Requirement already satisfied: packaging in /home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages (24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ninja packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn\n",
      "  Using cached flash_attn-2.7.4.post1.tar.gz (6.0 MB)\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages (from flash-attn) (2.6.0+cu118)\n",
      "Collecting einops (from flash-attn)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages (from torch->flash-attn) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages (from torch->flash-attn) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages (from torch->flash-attn) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages (from torch->flash-attn) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages (from torch->flash-attn) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages (from torch->flash-attn) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages (from torch->flash-attn) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages (from torch->flash-attn) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages (from torch->flash-attn) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages (from torch->flash-attn) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages (from torch->flash-attn) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages (from torch->flash-attn) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages (from torch->flash-attn) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages (from torch->flash-attn) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages (from torch->flash-attn) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages (from torch->flash-attn) (11.8.86)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages (from torch->flash-attn) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages (from torch->flash-attn) (77.0.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages (from torch->flash-attn) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages (from jinja2->torch->flash-attn) (3.0.2)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Building wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for flash-attn \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[195 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m torch.__version__  = 2.6.0+cu118\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m /home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/setuptools/dist.py:760: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m         Please consider removing the following classifiers in favor of a SPDX license expression:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         License :: OSI Approved :: BSD License\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m         See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n",
      "  \u001b[31m   \u001b[0m         ********************************************************************************\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m !!\n",
      "  \u001b[31m   \u001b[0m   self._finalize_license_expression()\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m Guessing wheel URL:  https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu11torch2.6cxx11abiFALSE-cp312-cp312-linux_x86_64.whl\n",
      "  \u001b[31m   \u001b[0m Precompiled wheel not found. Building from source...\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/fused_softmax.py -> build/lib.linux-x86_64-cpython-312/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/bert_padding.py -> build/lib.linux-x86_64-cpython-312/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_og.py -> build/lib.linux-x86_64-cpython-312/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_blocksparse_attention.py -> build/lib.linux-x86_64-cpython-312/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/__init__.py -> build/lib.linux-x86_64-cpython-312/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_interface.py -> build/lib.linux-x86_64-cpython-312/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton.py -> build/lib.linux-x86_64-cpython-312/flash_attn\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_blocksparse_attn_interface.py -> build/lib.linux-x86_64-cpython-312/flash_attn\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/hopper\n",
      "  \u001b[31m   \u001b[0m copying hopper/test_kvcache.py -> build/lib.linux-x86_64-cpython-312/hopper\n",
      "  \u001b[31m   \u001b[0m copying hopper/test_util.py -> build/lib.linux-x86_64-cpython-312/hopper\n",
      "  \u001b[31m   \u001b[0m copying hopper/generate_kernels.py -> build/lib.linux-x86_64-cpython-312/hopper\n",
      "  \u001b[31m   \u001b[0m copying hopper/benchmark_split_kv.py -> build/lib.linux-x86_64-cpython-312/hopper\n",
      "  \u001b[31m   \u001b[0m copying hopper/benchmark_flash_attention_fp8.py -> build/lib.linux-x86_64-cpython-312/hopper\n",
      "  \u001b[31m   \u001b[0m copying hopper/benchmark_attn.py -> build/lib.linux-x86_64-cpython-312/hopper\n",
      "  \u001b[31m   \u001b[0m copying hopper/test_flash_attn.py -> build/lib.linux-x86_64-cpython-312/hopper\n",
      "  \u001b[31m   \u001b[0m copying hopper/test_attn_kvcache.py -> build/lib.linux-x86_64-cpython-312/hopper\n",
      "  \u001b[31m   \u001b[0m copying hopper/__init__.py -> build/lib.linux-x86_64-cpython-312/hopper\n",
      "  \u001b[31m   \u001b[0m copying hopper/flash_attn_interface.py -> build/lib.linux-x86_64-cpython-312/hopper\n",
      "  \u001b[31m   \u001b[0m copying hopper/padding.py -> build/lib.linux-x86_64-cpython-312/hopper\n",
      "  \u001b[31m   \u001b[0m copying hopper/setup.py -> build/lib.linux-x86_64-cpython-312/hopper\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/layers/patch_embed.py -> build/lib.linux-x86_64-cpython-312/flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/layers/__init__.py -> build/lib.linux-x86_64-cpython-312/flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/layers/rotary.py -> build/lib.linux-x86_64-cpython-312/flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/distributed.py -> build/lib.linux-x86_64-cpython-312/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/generation.py -> build/lib.linux-x86_64-cpython-312/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/benchmark.py -> build/lib.linux-x86_64-cpython-312/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/pretrained.py -> build/lib.linux-x86_64-cpython-312/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/utils/__init__.py -> build/lib.linux-x86_64-cpython-312/flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/layer_norm.py -> build/lib.linux-x86_64-cpython-312/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/fused_dense.py -> build/lib.linux-x86_64-cpython-312/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/rms_norm.py -> build/lib.linux-x86_64-cpython-312/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/activations.py -> build/lib.linux-x86_64-cpython-312/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/__init__.py -> build/lib.linux-x86_64-cpython-312/flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/losses/cross_entropy.py -> build/lib.linux-x86_64-cpython-312/flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/losses/__init__.py -> build/lib.linux-x86_64-cpython-312/flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/gptj.py -> build/lib.linux-x86_64-cpython-312/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/baichuan.py -> build/lib.linux-x86_64-cpython-312/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/falcon.py -> build/lib.linux-x86_64-cpython-312/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/llama.py -> build/lib.linux-x86_64-cpython-312/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/vit.py -> build/lib.linux-x86_64-cpython-312/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/btlm.py -> build/lib.linux-x86_64-cpython-312/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/gpt.py -> build/lib.linux-x86_64-cpython-312/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/bert.py -> build/lib.linux-x86_64-cpython-312/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/bigcode.py -> build/lib.linux-x86_64-cpython-312/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/__init__.py -> build/lib.linux-x86_64-cpython-312/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/gpt_neox.py -> build/lib.linux-x86_64-cpython-312/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/models/opt.py -> build/lib.linux-x86_64-cpython-312/flash_attn/models\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/utils.py -> build/lib.linux-x86_64-cpython-312/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/interface_fa.py -> build/lib.linux-x86_64-cpython-312/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/fwd_decode.py -> build/lib.linux-x86_64-cpython-312/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/bwd_prefill.py -> build/lib.linux-x86_64-cpython-312/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/bench.py -> build/lib.linux-x86_64-cpython-312/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/bwd_ref.py -> build/lib.linux-x86_64-cpython-312/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/fwd_ref.py -> build/lib.linux-x86_64-cpython-312/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/__init__.py -> build/lib.linux-x86_64-cpython-312/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/fwd_prefill.py -> build/lib.linux-x86_64-cpython-312/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/test.py -> build/lib.linux-x86_64-cpython-312/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/flash_attn_triton_amd/interface_torch.py -> build/lib.linux-x86_64-cpython-312/flash_attn/flash_attn_triton_amd\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/modules/mha.py -> build/lib.linux-x86_64-cpython-312/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/modules/embedding.py -> build/lib.linux-x86_64-cpython-312/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/modules/block.py -> build/lib.linux-x86_64-cpython-312/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/modules/__init__.py -> build/lib.linux-x86_64-cpython-312/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/modules/mlp.py -> build/lib.linux-x86_64-cpython-312/flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-312/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/layer_norm.py -> build/lib.linux-x86_64-cpython-312/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/k_activations.py -> build/lib.linux-x86_64-cpython-312/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/cross_entropy.py -> build/lib.linux-x86_64-cpython-312/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/__init__.py -> build/lib.linux-x86_64-cpython-312/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/mlp.py -> build/lib.linux-x86_64-cpython-312/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/rotary.py -> build/lib.linux-x86_64-cpython-312/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m copying flash_attn/ops/triton/linear.py -> build/lib.linux-x86_64-cpython-312/flash_attn/ops/triton\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 486, in run\n",
      "  \u001b[31m   \u001b[0m   File \"/usr/lib/python3.12/urllib/request.py\", line 240, in urlretrieve\n",
      "  \u001b[31m   \u001b[0m     with contextlib.closing(urlopen(url, data)) as fp:\n",
      "  \u001b[31m   \u001b[0m                             ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/usr/lib/python3.12/urllib/request.py\", line 215, in urlopen\n",
      "  \u001b[31m   \u001b[0m     return opener.open(url, data, timeout)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/usr/lib/python3.12/urllib/request.py\", line 521, in open\n",
      "  \u001b[31m   \u001b[0m     response = meth(req, response)\n",
      "  \u001b[31m   \u001b[0m                ^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/usr/lib/python3.12/urllib/request.py\", line 630, in http_response\n",
      "  \u001b[31m   \u001b[0m     response = self.parent.error(\n",
      "  \u001b[31m   \u001b[0m                ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/usr/lib/python3.12/urllib/request.py\", line 559, in error\n",
      "  \u001b[31m   \u001b[0m     return self._call_chain(*args)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/usr/lib/python3.12/urllib/request.py\", line 492, in _call_chain\n",
      "  \u001b[31m   \u001b[0m     result = func(*args)\n",
      "  \u001b[31m   \u001b[0m              ^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/usr/lib/python3.12/urllib/request.py\", line 639, in http_error_default\n",
      "  \u001b[31m   \u001b[0m     raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "  \u001b[31m   \u001b[0m urllib.error.HTTPError: HTTP Error 404: Not Found\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m During handling of the above exception, another exception occurred:\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 353, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 335, in main\n",
      "  \u001b[31m   \u001b[0m     json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 251, in build_wheel\n",
      "  \u001b[31m   \u001b[0m     return _build_backend().build_wheel(wheel_directory, config_settings,\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/setuptools/build_meta.py\", line 438, in build_wheel\n",
      "  \u001b[31m   \u001b[0m     return _build(['bdist_wheel', '--dist-info-dir', str(metadata_directory)])\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/setuptools/build_meta.py\", line 426, in _build\n",
      "  \u001b[31m   \u001b[0m     return self._build_with_temp_dir(\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/setuptools/build_meta.py\", line 407, in _build_with_temp_dir\n",
      "  \u001b[31m   \u001b[0m     self.run_setup()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/setuptools/build_meta.py\", line 522, in run_setup\n",
      "  \u001b[31m   \u001b[0m     super().run_setup(setup_script=setup_script)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/setuptools/build_meta.py\", line 320, in run_setup\n",
      "  \u001b[31m   \u001b[0m     exec(code, locals())\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 526, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/setuptools/__init__.py\", line 117, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/setuptools/_distutils/core.py\", line 186, in setup\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/setuptools/_distutils/core.py\", line 202, in run_commands\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1002, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/setuptools/dist.py\", line 1105, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1021, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 503, in run\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/setuptools/command/bdist_wheel.py\", line 370, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(\"build\")\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/setuptools/_distutils/cmd.py\", line 357, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/setuptools/dist.py\", line 1105, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1021, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/setuptools/_distutils/command/build.py\", line 135, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd_name)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/setuptools/_distutils/cmd.py\", line 357, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/setuptools/dist.py\", line 1105, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/setuptools/_distutils/dist.py\", line 1021, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/setuptools/command/build_ext.py\", line 99, in run\n",
      "  \u001b[31m   \u001b[0m     _build_ext.run(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/setuptools/_distutils/command/build_ext.py\", line 368, in run\n",
      "  \u001b[31m   \u001b[0m     self.build_extensions()\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 552, in build_extensions\n",
      "  \u001b[31m   \u001b[0m     _check_cuda_version(compiler_name, compiler_version)\n",
      "  \u001b[31m   \u001b[0m   File \"/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/torch/utils/cpp_extension.py\", line 447, in _check_cuda_version\n",
      "  \u001b[31m   \u001b[0m     raise RuntimeError(CUDA_MISMATCH_MESSAGE.format(cuda_str_version, torch.version.cuda))\n",
      "  \u001b[31m   \u001b[0m RuntimeError:\n",
      "  \u001b[31m   \u001b[0m The detected CUDA version (12.8) mismatches the version that was used to compile\n",
      "  \u001b[31m   \u001b[0m PyTorch (11.8). Please make sure to use the same CUDA versions.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[31m  ERROR: Failed building wheel for flash-attn\u001b[0m\u001b[31m\n",
      "\u001b[0mFailed to build flash-attn\n",
      "\u001b[31mERROR: Could not build wheels for flash-attn, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use a pipeline as a high-level helper\n",
    "# from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\"automatic-speech-recognition\", model=\"microsoft/Phi-4-multimodal-instruct\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g2/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/transformers/models/auto/image_processing_auto.py:602: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load model directly\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmicrosoft/Phi-4-multimodal-instruct\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:568\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    566\u001b[39m     \u001b[38;5;28mcls\u001b[39m.register(config.\u001b[34m__class__\u001b[39m, model_class, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    567\u001b[39m     model_class = add_generation_mixin_to_remote_model(model_class)\n\u001b[32m--> \u001b[39m\u001b[32m568\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    572\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:272\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    270\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    274\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4395\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4393\u001b[39m config = copy.deepcopy(config)  \u001b[38;5;66;03m# We do not want to modify the config inplace in from_pretrained.\u001b[39;00m\n\u001b[32m   4394\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[33m\"\u001b[39m\u001b[33m_attn_implementation_autoset\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m4395\u001b[39m     config = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_autoset_attn_implementation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_flash_attention_2\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_flash_attention_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\n\u001b[32m   4397\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4399\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ContextManagers(model_init_context):\n\u001b[32m   4400\u001b[39m     \u001b[38;5;66;03m# Let's make sure we don't run the init function of buffer modules\u001b[39;00m\n\u001b[32m   4401\u001b[39m     model = \u001b[38;5;28mcls\u001b[39m(config, *model_args, **model_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:2112\u001b[39m, in \u001b[36mPreTrainedModel._autoset_attn_implementation\u001b[39m\u001b[34m(cls, config, use_flash_attention_2, torch_dtype, device_map, check_device_map)\u001b[39m\n\u001b[32m   2109\u001b[39m     config._attn_implementation = \u001b[33m\"\u001b[39m\u001b[33mflash_attention_2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config._attn_implementation == \u001b[33m\"\u001b[39m\u001b[33mflash_attention_2\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2112\u001b[39m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_and_enable_flash_attn_2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhard_check_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2117\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_device_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_device_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2118\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2119\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m requested_attn_implementation == \u001b[33m\"\u001b[39m\u001b[33mflex_attention\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2120\u001b[39m     config = \u001b[38;5;28mcls\u001b[39m._check_and_enable_flex_attn(config, hard_check_only=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM-GGC-Upgrade/src/LLMs/Image-Text-to-Text Models/my_gpu_project/venv/lib/python3.12/site-packages/transformers/modeling_utils.py:2253\u001b[39m, in \u001b[36mPreTrainedModel._check_and_enable_flash_attn_2\u001b[39m\u001b[34m(cls, config, torch_dtype, device_map, check_device_map, hard_check_only)\u001b[39m\n\u001b[32m   2249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m   2250\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreface\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m the package flash_attn is not supported on Ascend NPU. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecommend_message_npu\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   2251\u001b[39m         )\n\u001b[32m   2252\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2253\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreface\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m the package flash_attn seems to be not installed. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minstall_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   2255\u001b[39m flash_attention_version = version.parse(importlib.metadata.version(\u001b[33m\"\u001b[39m\u001b[33mflash_attn\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m   2256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.version.cuda:\n",
      "\u001b[31mImportError\u001b[39m: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2."
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-4-multimodal-instruct\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
