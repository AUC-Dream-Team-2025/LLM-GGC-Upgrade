{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gM0YtadPUH69",
        "outputId": "25a12b71-e5f1-441f-814c-7fdc993425ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.13)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z92KUj4oTFGg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c4e2a7a-261c-450a-991e-bb8f5ab52d2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "<ipython-input-4-0e440d044f12>:65: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "s s\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoImageProcessor, AutoModel, T5ForConditionalGeneration, T5Tokenizer\n",
        "from PIL import Image\n",
        "import requests\n",
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"hiyouga/geometry3k\")\n",
        "\n",
        "class MultiModalT5Model(nn.Module):\n",
        "    def __init__(self, vision_model_name, t5_model_name, device):\n",
        "        super().__init__()\n",
        "        self.device = device\n",
        "        # Load the vision components.\n",
        "        self.vision_processor = AutoImageProcessor.from_pretrained(vision_model_name)\n",
        "        self.vision_encoder = AutoModel.from_pretrained(vision_model_name).to(device)\n",
        "\n",
        "        # Load the T5 components.\n",
        "        self.tokenizer = T5Tokenizer.from_pretrained(t5_model_name)\n",
        "        self.t5 = T5ForConditionalGeneration.from_pretrained(t5_model_name).to(device)\n",
        "\n",
        "        # T5 uses d_model as its hidden size.\n",
        "        vision_hidden_size = self.vision_encoder.config.hidden_size\n",
        "        t5_hidden_size = self.t5.config.d_model\n",
        "\n",
        "        # Add a projection layer if the sizes differ.\n",
        "        if vision_hidden_size != t5_hidden_size:\n",
        "            self.projection = nn.Linear(vision_hidden_size, t5_hidden_size).to(device)\n",
        "        else:\n",
        "            self.projection = nn.Identity()\n",
        "\n",
        "    def forward(self, image, prompt, max_length=50):\n",
        "        # Process the image and obtain image embeddings.\n",
        "        vision_inputs = self.vision_processor(images=image, return_tensors=\"pt\").to(self.device)\n",
        "        vision_outputs = self.vision_encoder(**vision_inputs)\n",
        "        image_embeds = vision_outputs.last_hidden_state  # (batch, img_seq_len, vision_hidden_size)\n",
        "        projected_image_embeds = self.projection(image_embeds)  # (batch, img_seq_len, t5_hidden_size)\n",
        "\n",
        "        # Process the prompt text.\n",
        "        prompt_tokens = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True).to(self.device)\n",
        "        # Use T5's shared embedding layer to get text embeddings.\n",
        "        prompt_embeds = self.t5.shared(prompt_tokens.input_ids)  # (batch, text_seq_len, t5_hidden_size)\n",
        "\n",
        "        # Concatenate image embeddings (as a prefix) with prompt embeddings.\n",
        "        combined_embeds = torch.cat([projected_image_embeds, prompt_embeds], dim=1)\n",
        "\n",
        "        # Generate output text using the T5 model.\n",
        "        generated_ids = self.t5.generate(inputs_embeds=combined_embeds, max_length=max_length)\n",
        "        generated_text = self.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        return generated_text\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    vision_model_name = \"facebook/dinov2-base\"\n",
        "    t5_model_name = \"t5-small\"\n",
        "\n",
        "    multimodal_model = MultiModalT5Model(vision_model_name, t5_model_name, device)\n",
        "    multimodal_model.eval()\n",
        "\n",
        "    dataset = ds['test']\n",
        "    image = dataset[0]['images']\n",
        "    prompt = dataset[0]['problem']\n",
        "\n",
        "    # Use autocast for mixed precision (reduces memory without full quantization)\n",
        "    with torch.cuda.amp.autocast():\n",
        "        with torch.no_grad():\n",
        "            output_text = multimodal_model(image, prompt, max_length=500)\n",
        "            print(output_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pk4Ry-0dUSfc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_on_dataset(model, dataset, device=\"cuda:0\", max_samples=None, max_length=50):\n",
        "    \"\"\"\n",
        "    Evaluate the given multimodal model on the dataset.\n",
        "\n",
        "    Each sample in the dataset should have:\n",
        "      - \"images\": a PIL image,\n",
        "      - \"problem\": the problem statement (a dynamic prompt),\n",
        "      - \"ground_truth\" or \"answer\": the expected output.\n",
        "\n",
        "    Args:\n",
        "      model: The multimodal image-text model to evaluate. It should have a callable\n",
        "             interface like model(image, prompt, max_length=...) that returns decoded text.\n",
        "      dataset: A dataset (e.g., a Hugging Face dataset) where each sample is a dict.\n",
        "      device: The device to run the evaluation on (default \"cuda:0\").\n",
        "      max_samples: Optional integer limit on the number of samples to evaluate.\n",
        "      max_length: Maximum length for generated text.\n",
        "\n",
        "    Returns:\n",
        "      A list of dictionaries with keys \"problem\", \"prediction\", and \"ground_truth\".\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for idx, sample in enumerate(dataset):\n",
        "        if max_samples is not None and idx >= max_samples:\n",
        "            break\n",
        "\n",
        "        # Build the dynamic prompt using the sample's problem statement.\n",
        "        prompt = (\n",
        "            \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n",
        "            \"<|im_start|>user\\n\" + sample[\"problem\"] + \"\\n\" +\n",
        "            \"<|vision_start|><|image_pad|><|vision_end|>\" +\n",
        "            \"Please answer the above problem using the given image.<|im_end|>\\n\"\n",
        "            \"<|endoftext|>\"\n",
        "        )\n",
        "\n",
        "        image = sample[\"images\"]\n",
        "\n",
        "        ground_truth = sample.get(\"ground_truth\", sample.get(\"answer\", \"\"))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output_text = model(image, prompt, max_length=max_length)\n",
        "\n",
        "        results.append({\n",
        "            \"problem\": sample[\"problem\"],\n",
        "            \"prediction\": output_text,\n",
        "            \"ground_truth\": ground_truth\n",
        "        })\n",
        "\n",
        "        if idx % 10 == 0:\n",
        "            print(f\"Processed sample {idx}\")\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "0NjMlxexgkVP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = evaluate_model_on_dataset(\n",
        "    model=multimodal_model,\n",
        "    dataset=dataset,\n",
        "    device=\"cuda:0\",\n",
        "    max_samples=10,\n",
        "    max_length=500\n",
        ")\n",
        "\n",
        "# Print the results.\n",
        "correct_count = 0\n",
        "\n",
        "for res in results:\n",
        "    print(\"Problem:\", res[\"problem\"])\n",
        "    print(\"Prediction:\", res[\"prediction\"])\n",
        "    print(\"Ground Truth:\", res[\"ground_truth\"])\n",
        "    print(\"=\" * 40)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7HwOhNzVgsvM",
        "outputId": "e4c06737-e6d8-4274-c2e5-63ba877bd51c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed sample 0\n",
            "Problem: <image>In the figure, $\\overline{A D}$ is perpendicular to $\\overline{B C}$ and $\\overline{A B}$ is perpendicular to $\\overline{A C}$. What is $B C ?$\n",
            "Prediction: \n",
            "Ground Truth: C\n",
            "========================================\n",
            "Problem: <image>In $\\odot M$, $FL=24,HJ=48$, and $m \\widehat {HP}=65$. Find $m \\widehat {PJ}$.\n",
            "Prediction: \n",
            "Ground Truth: B\n",
            "========================================\n",
            "Problem: <image>Find $G I$ if $G H=9, G K=6,$ and $K J=4$\n",
            "Prediction: ........................................................\n",
            "Ground Truth: D\n",
            "========================================\n",
            "Problem: <image>in $\\triangle XYZ$, $P$ is the centroid, $KP=3$, and $XJ=8$. Find $YJ$.\n",
            "Prediction: s s s s s s s s s s s s. s s s s. s s. s s. s s. s s. s s. s s. s s. s s. s s s s. s s s s s s s s s\n",
            "Ground Truth: C\n",
            "========================================\n",
            "Problem: <image>A regular pentagon and a square share a mutual vertex $X$. The sides $\\overline{X Y}$ and $\\overline{X Z}$ are sides of a third regular polygon with a vertex at $X .$ How many sides does this polygon have?\n",
            "Prediction: ........................................................\n",
            "Ground Truth: B\n",
            "========================================\n",
            "Problem: <image>Use parallelogram $JKLM$ to find $m \\angle KJL$.\n",
            "Prediction: s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s\n",
            "Ground Truth: A\n",
            "========================================\n",
            "Problem: <image>Find x.\n",
            "Prediction: s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s\n",
            "Ground Truth: C\n",
            "========================================\n",
            "Problem: <image>Find x. Assume that segments that appear to be tangent are tangent.\n",
            "Prediction: ........................................................\n",
            "Ground Truth: B\n",
            "========================================\n",
            "Problem: <image>Find x to the nearest tenth. Assume that segments that appear to be tangent are tangent.\n",
            "Prediction: \n",
            "Ground Truth: C\n",
            "========================================\n",
            "Problem: <image>Find $m\\angle CAM$\n",
            "Prediction: ........................................................\n",
            "Ground Truth: B\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SaADDIAYfCZa"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}