{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59c03147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages/bitsandbytes-0.41.0-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (4.50.3)\n",
      "Requirement already satisfied: peft in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (0.15.1)\n",
      "Requirement already satisfied: datasets in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (2.18.0)\n",
      "Requirement already satisfied: accelerate in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: filelock in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from transformers) (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from peft) (7.0.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from peft) (2.6.0+cu124)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from aiohttp->datasets) (6.3.2)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: networkx in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (70.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers peft datasets accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ce53bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/g2/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages/transformers/quantizers/auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     11\u001b[39m accelerator = Accelerator()\n\u001b[32m     13\u001b[39m quantization_config = BitsAndBytesConfig(\n\u001b[32m     14\u001b[39m     load_in_4bit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     15\u001b[39m     llm_int8_enable_fp32_cpu_offload=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     16\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m model = \u001b[43mAutoModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mopenbmb/MiniCPM-V-2_6-int4\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moffload\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     24\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m tokenizer = AutoTokenizer.from_pretrained(\u001b[33m'\u001b[39m\u001b[33mopenbmb/MiniCPM-V-2_6-int4\u001b[39m\u001b[33m'\u001b[39m, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     26\u001b[39m tokenizer.pad_token = tokenizer.eos_token\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:568\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    566\u001b[39m     \u001b[38;5;28mcls\u001b[39m.register(config.\u001b[34m__class__\u001b[39m, model_class, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    567\u001b[39m     model_class = add_generation_mixin_to_remote_model(model_class)\n\u001b[32m--> \u001b[39m\u001b[32m568\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    572\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages/transformers/modeling_utils.py:272\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    270\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    274\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages/transformers/modeling_utils.py:4434\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4432\u001b[39m \u001b[38;5;66;03m# Prepare the full device map\u001b[39;00m\n\u001b[32m   4433\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4434\u001b[39m     device_map = \u001b[43m_get_device_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\n\u001b[32m   4436\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4438\u001b[39m \u001b[38;5;66;03m# Finalize model weight initialization\u001b[39;00m\n\u001b[32m   4439\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m from_tf:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages/transformers/modeling_utils.py:1338\u001b[39m, in \u001b[36m_get_device_map\u001b[39m\u001b[34m(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_modules)\u001b[39m\n\u001b[32m   1335\u001b[39m     device_map = infer_auto_device_map(model, dtype=target_dtype, **device_map_kwargs)\n\u001b[32m   1337\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1338\u001b[39m         \u001b[43mhf_quantizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1340\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1341\u001b[39m     tied_params = find_tied_parameters(model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/LLM-GGC-Upgrade/.env/lib/python3.12/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:104\u001b[39m, in \u001b[36mBnb4BitHfQuantizer.validate_environment\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    102\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values() \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map_without_lm_head.values():\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    105\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    106\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    107\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    108\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`from_pretrained`. Check \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    109\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    110\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfor more details. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    111\u001b[39m         )\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m version.parse(importlib.metadata.version(\u001b[33m\"\u001b[39m\u001b[33mbitsandbytes\u001b[39m\u001b[33m\"\u001b[39m)) < version.parse(\u001b[33m\"\u001b[39m\u001b[33m0.39.0\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    115\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    116\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m make sure you have the latest version of `bitsandbytes` installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    117\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "source": [
    "# test.py\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True\n",
    ")\n",
    "\n",
    "model = AutoModel.from_pretrained(\n",
    "    'openbmb/MiniCPM-V-2_6-int4',\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    offload_folder=\"offload\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained('openbmb/MiniCPM-V-2_6-int4', trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "model, tokenizer = accelerator.prepare(model, tokenizer)\n",
    "\n",
    "# Define LoRA configuration with target_modules\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",  # Task type (e.g., Causal Language Modeling)\n",
    "    inference_mode=False,  # Set to True for inference-only use\n",
    "    r=8,  # Low-rank dimension\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    lora_dropout=0.1,  # Dropout for LoRA layers\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Specify target modules for LoRA\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "dataset = load_dataset(\"U4R/ChartX\", split=\"validation\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Define training arguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "\n",
    "    output_dir=\"./lora-fine-tuned\", # Output directory\n",
    "\n",
    "    per_device_train_batch_size=8,\n",
    "\n",
    "    num_train_epochs=3,\n",
    "\n",
    "    logging_dir=\"./logs\",\n",
    "\n",
    "    logging_steps=10,\n",
    "\n",
    "    save_steps=500,\n",
    "\n",
    "    save_total_limit=2,\n",
    "\n",
    "    evaluation_strategy=\"steps\",\n",
    "\n",
    "    eval_steps=500,\n",
    "\n",
    "    learning_rate=5e-4, # Smaller learning rate for LoRA\n",
    "\n",
    "    ddp_find_unused_parameters=False,  # Helps avoid certain DDP bugs\n",
    "\n",
    ")\n",
    "\n",
    "# Define a Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "\n",
    "    model=peft_model,\n",
    "\n",
    "    args=training_args,\n",
    "\n",
    "    train_dataset=tokenized_dataset,\n",
    "    \n",
    "\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "peft_model.save_pretrained(\"./lora-fine-tuned\")\n",
    "\n",
    "# Load the fine-tuned model for inference\n",
    "\n",
    "fine_tuned_model = PeftModel.from_pretrained(model, \"./lora-fine-tuned\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc03a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "image = Image.open('xx.jpg').convert('RGB')\n",
    "question = 'What is in the image?'\n",
    "msgs = [{'role': 'user', 'content': [image, question]}]\n",
    "\n",
    "res = model.chat(\n",
    "    image=None,\n",
    "    msgs=msgs,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(res)\n",
    "\n",
    "## if you want to use streaming, please make sure sampling=True and stream=True\n",
    "## the model.chat will return a generator\n",
    "res = model.chat(\n",
    "    image=None,\n",
    "    msgs=msgs,\n",
    "    tokenizer=tokenizer,\n",
    "    sampling=True,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "generated_text = \"\"\n",
    "for new_text in res:\n",
    "    generated_text += new_text\n",
    "    print(new_text, flush=True, end='')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
